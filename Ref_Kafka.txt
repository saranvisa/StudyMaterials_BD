1. What are we using for publish/produce data to a topic. 
		a. Logstash/Flume/Kafka connect/anything else ? 
2. Spark streaming is a consumer right? 
3. To run the program in local (using IntelliJ), do I need to install zookeeper, kafka client and "all other tools"  in my local machine (in addition to cluster)? Yes
4. If so, how to install zookeeper, kafka client and 'all other tools' in windows local? 
5. 


------
1. Reference (quick start) : https://kafka.apache.org/quickstart
https://kafka.apache.org/documentation/
https://github.com/Landoop/stream-reactor			## Kafka Connector to all other services like Elasticsearch, hive, mongo, cassandra, etc
https://gist.github.com/dgadiraju/c4ed3195e563779e97a1658598269652
Reference: http://www.godatafy.com/category/tech-blog/kafka-tech-blog/
Reference: Very useful Youtube Channel: "bigdata simplified

2. Learning Journal: 
	Github		: https://github.com/LearningJournal/ApacheKafkaTutorials/tree/master/ProducerExamples
	Website		: https://www.learningjournal.guru/courses/kafka/kafka-foundation-training/
	Youtube		: https://www.youtube.com/watch?v=twvdT6A1eeE&list=PLkz1SCf5iB4enAR00Z46JwY9GGkaS2NON&index=10
	
	*** Very clear basic info ***
	https://www.youtube.com/watch?v=udnX21__SuU
	It will cover following things.
		1. Producer
		2. Consumer
		3. Broker
		4. Cluster
		5. Topic
		6. Partitions
		7. Offset
		8. Consumer groups

2. ITVersity Complete reference including Logstash (ELK)
	https://www.youtube.com/watch?v=NhDM9dFnTA8								## Only intro to kafka and ELK 
	https://www.youtube.com/playlist?list=PLf0swTFhTI8rM5AdWXObgQ-aN5gYyOfKC				## Complete playlist 
	https://www.youtube.com/watch?v=EZb3ueApt7A&list=PLf0swTFhTI8rM5AdWXObgQ-aN5gYyOfKC&index=3		## Overview of streaming technology
	https://www.youtube.com/watch?v=NhDM9dFnTA8&list=PLf0swTFhTI8rM5AdWXObgQ-aN5gYyOfKC 			## Overview of streaming technology
	
3. Limitations of Flume and Kafka. Also to understand why do we need both Flume and Kafka
	https://www.youtube.com/watch?v=V3smJMQaArI&list=PLf0swTFhTI8pZ6fj9OmHKuESF0yUL-wkb&index=14
	
4. Kafka security
		https://kafka.apache.org/documentation/#security
		
5. Kafka - Producer example 
	https://www.tutorialspoint.com/apache_kafka/apache_kafka_simple_producer_example.htm
	
6. JavaDoc:
		https://kafka.apache.org/10/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html
		https://kafka.apache.org/10/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html
	
##########################################################################
Frequently Used Commands/Tips
##########################################################################

** Supports Java and Scala programs
** "Source"(WebServer/DB)-> "3rd party tool"(Logstash/Flume/Kafka connect)-> "Kafka topic" -> "3rd party tool" -> "target"(NoSQL DB,Hadoop,etc)
** For 3rd party tool - In addtion to Logstash, Flume, Kafka Connect, We can also use Custom Apps (using produer API)
** Logstash is a dynamic data collection pipeline with an extensible plugin eco system and strong elastic search synergy

Tips:
1. ZooKeeper defalt port: 2181
2. Kafka default port : 9092 
3. Use -daemoan option to execute kafka in background	
4. API:
	a. There are 5 core APIs
		1. Producer API
		2. Consumer API
		3. Kafka Connect API
		4. Streaming API
		5. AdminClient API 
	b. We can use Kafka publisher api and directly publish the messages to topic 
5. Important config files: 
	a. config/zookeeper.properties
	b. config/server.properties
		* The broker.id property is the unique and permanent name of each node in the cluster
		* log.dirs=/tmp/kafka-logs-0 ## This folder contains a log file this is the messaging queue 
6. Publisher/Producer: are processes that publish data (push messages) to the message queue
	a. It can be a tipical Scala or Java based program which uses publisher API 
	b. Kafka Connect 
	c. Logstash
	d. Flume 
7. Subscriber/Consumer: Are processes that read from the message queue 		
	a. It can be a Scala or Java based program which uses Consumer API 
	b. Spark streaming 
	c. Kafka Connect 
	d. etc 
8. Pre-request:
	a. Check the Version compatibility of Kafka, Spark Streaming, target (Hbase, bigdata, etc) 
	b. Go to Kafka library path in hadoop -> (Ex: In horton works) -> "cd /usr/hdp/2.6.5.0-292/kafka/libs" -> kafka jar will be here
	c. kafka_2.11-1.0.0.2.6.5.0-292-source.jar.asc  ## Below are compatible version# 
		a. 2.11 is for scala
		b. 1.0.0 is for Kafka 
		c. 2.6.5.0 is for Hadooop 
		d. Note: Scala 2.11 is not compatiable with JDK 1.9, so use JDK 1.8
		e. For SBT, use 0.13.17 
		
9. We need to install Kafka server and kafaka clients as needed. Below is an example for both:
		a. kafka_2.11-1.0.0.2.6.5.0-292-source.jar.asc		## kafka-Server 
		b. kafka-clients-1.0.0.2.6.5.0-292.jar 				## kafaka-clients - is compatible with multiple scala version, so scala version is not mentioned

10. For SBT or Maven, search for the repositories and library dependencies in the google. 
	a. Ex: Search for Maven repositories for cloudera
	b. Ex: Search for Maven repositories for Kafka 
	
11. Serialization: The activity of converting java objects into "array of bytes" is called serialization. Because Kafka doesn't have any meaning for the data, it is just an array 	  of 	bytes for kafka. 

	https://www.youtube.com/watch?v=twvdT6A1eeE&list=PLkz1SCf5iB4enAR00Z46JwY9GGkaS2NON&index=10
	Ex: 
	
	String key = "Key1";
	String value = "Value-1";
	String topicName = "SimpleProducerTopic";
	
	Properties props = new Properties();
	props.put("bootstrap.servers", "localhost:9092,localhost:9093");
	## Below we are trying to send String Key and String Value but Kafka understands only "array of bytes". So using the below class to convert the string to array of bytes 
	## This is called serialization
	## Kafka also provides other serializers called IntSerializer and DoubleSerializer
	props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

	## Below the <String, String> -- The first one is key and second one is value , so we need to change it based on the data type of key and value 
	Producer<String, String> producer = new KafkaProducer<>(props);           				### Step 1: To create producer object, should use the above properties (props)
	ProducerRecord<String, String> record = new ProducerRecord<>(topicName,key,value);		### Step 2: producer Record object, should have topic, key and value 
	producer.send(record);                													### Step 3: Send the record 
	producer.close();
	System.out.println("SimpleProducer Completed.");
	
12. 'Producer record' constructors in java: 	
	a. topic:
	b. partition (optional):
	c. timestamp (optional): if you send the timestamp it will take it otherwise, it will consider the timestamp when it reaches the broker by default
	d. Key (optional): Key is used to identify the partition. If key is not mentioned, then it will use default partition 
	e. Value: 
	
13. Properties:
	a. bootstrap.servers
	b. key.serializer 
	c. value.serializer
	And Others.... (above 3 are mandatory)

	Properties props = new Properties();
	props.put("bootstrap.servers", "localhost:9092,localhost:9093");
	props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
	props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	
14. Producer workflow: https://www.youtube.com/watch?v=_RrgVDTd38M&list=PLkz1SCf5iB4enAR00Z46JwY9GGkaS2NON&index=11

15. 
	
----	
1. ITVersity: Kafka bin path: /usr/hdp/2.6.5.0-292/kafka/bin
	So in ITVersity, update the .profile (or) .bash_profile with:  export PATH=$PATH:/usr/hdp/2.6.5.0-292/kafka/bin
	
2. Deprecated property as per Apache flume : 
	a. broker-list: This can be a partial list of brokers, but we recommend at least two for HA
	b. Use kafka.topic instead of topic	
	c. Use kafka.consumer.group.id instead of groupId
	d. Use kafka.consumer.auto.offset.reset instead of readSmallestOffset
	
3. What is Kafka?
	It is an open source stream-processing software plantform developed by the Apache Souftware foundation, written in Scala and Java. It was initially developed as an internal product at LinkedIn and was Open-sourced and adopted by apache foundation. 
	
3.1. Features of Kafka?
	a. Highly scalable (achieved by Partition) 
	b. Fault Tolerant	(achieved by replication factor)
	c. Low Latency 
	d. High Throughput 

4. What is Kafka broker?
	A kafka cluster consists of one or more servers (Kafka Brokers) which are running kafka. Producers are processes that publish data (push mesage) into kafka topic within the broker. A consumer of topics pulls messages of a kafka topic 
	
4.1. What is Kafka Topic?
	Topic is a intermediate file structure managed by broker, where we can stream data from the producer to consumer. 
	
5. Zookeeper in kafka
		Zookeeper also plays a vital role for serving many other purposes, such as leader detection, configuration management, synchronization, detecting when a new node joins or leaves the cluster, etc. Future Kafka releases are planning to remove the zookeeper dependency but as of now it is an integral part of it
		
6. Kafka Parition
	
7. Kafka offset
		A Kafka topic receives messages across a distributed set of partitions where they are stored. Each partition maintains the messages it has received in a sequential order where they are identified by an offset, also known as a position
		
8. Commit in Kafka
			In Kafka releases through 0.8.1.1, consumers commit their offsets to ZooKeeper. ... Fortunately, Kafka now provides an ideal mechanism for storing consumer offsets. Consumers can commit their offsets in Kafka by writing them to a durable (replicated) and highly available topic.
	
9. Consumer Group?
	Consumer Group. Consumers can join a group by using the same group.id. The maximum parallelism of a group is that the number of consumers in the group ← no of partitions. Kafka assigns the partitions of a topic to the consumer in a group, so that each partition is consumed by exactly one consumer in the group.
	
10. Kafka APIs (Supports Java and Scala programs)
		a. Producer API
		b. Consumer API
		c. Streams API
		d. Connector API ** Internaly connector API will use Producer and Consumer APIs. ** 

11. Why Logstash?
	a. Seamless integration with ELK (Elasticsearch and Kibana) for easy metrics and visuallizaiton
	b. Extensible: Logstash works on a concept of plugins, it can be extended via plugins to parse various log formats and we can develop applications very easy and reliable way 
	c. Interoperable: Logstash works seamlessly  with other tools such as Kafka, HDFS, S3, Nagios, Ganglia , etc 
	d. Open source: Logstash is completely free to use and is an open source tool 
	
12.  How Logstash works?

	(Web Server) Source -> (Logstash/Flume) 3rd party tool -> Kafka topic -> 3rd party tool -> target (NoSQL DB, Hadoop, etc)

	Logstash is a "server-side" open source framework to ingest data, process it and send it to applications or "stashes" as they call it for further processing 

	To put it simply, Logstash is used to collect and transform data from multiple sources and pipelines it into different destinations with minimum settings with the use of the multiple plugins and extension

13. Sample Logstash use case
	a. Read data from log file
	b. Parse each message separately
	c. Get country code based on ip address
	d. Convert message into JSON
	e. Push each message into Kafka topic in real time 

14. Pub-Sub (Publish-Subscribe) Messaging - Topic:
	Pub/Sub messaging is a form of asynchronous service-to-service communication used in serverless and microservices architectures. In a pub/sub model, any message published to a topic is immediately receied by all of the subscirbers to the topic. 

##########################################################################
Kafka - Download and setup
##########################################################################

### Pre-request: Zookeeper , Kafka (any latest version) and JDK 1.7
https://www.youtube.com/watch?v=NhDM9dFnTA8&list=PLf0swTFhTI8rM5AdWXObgQ-aN5gYyOfKC 	## ITVersity: Streaming Pipelines - Kafka Core Concepts and Overview Of Logstash


		
		
## Single node configuration 

## Pre-request:
	a. Check the Version compatibility of Kafka, Spark Streaming, target (Hbase, bigdata, etc) 
	b. Go to Kafka library path in hadoop -> (Ex: In horton works) -> "cd /usr/hdp/2.6.5.0-292/kafka/libs" -> kafka jar will be here
	c. kafka_2.11-1.0.0.2.6.5.0-292-source.jar.asc  ## Below are compatible version# 
		a. 2.11 is for scala
		b. 1.0.0 is for Kafka 
		c. 2.6.5.0 is for Hadooop 
		d. Note: Scala 2.11 is not compatiable with JDK 1.9, so use JDK 1.8
		e. For SBT, use 0.13.17 
		
1. download & untar
	$ wget https://www-eu.apache.org/dist/kafka/2.1.0/kafka_2.11-2.1.0.tgz
	$ tar -xzf kafka_2.11-2.1.0.tgz
2. start zookeeper properties available in the kafka config folder
	
	$ cd /kafka/kafka_2.11-2.1.0
	
	## Update the config file as needed config/zookeeper.properties
	## Update the config file as needed config/server.properties
		a. config/server.properties:
			broker.id=0
			listeners=PLAINTEXT://<ip-address>:9092
			log.dirs=/tmp/kafka-logs-0
			zookeeper.connect = <ip-address>:2181,<ip-address2>:2181
			log.roll.hours=168
			log.retention.hours=168
		
		b. config/server-1.properties:
			broker.id=1
			listeners=PLAINTEXT://<ip-address>:9093
			log.dirs=/tmp/kafka-logs-1
			zookeeper.connect = <ip-address>:2181,<ip-address2>:2181
			log.roll.hours=168
			log.retention.hours=168
			
	$ sh bin/zookeeper-server-start.sh -daemon config/zookeeper.properties		## Using -daemoan to start the service in background
	
	## Validation for zookeeper
	$ telnet <ip-address> 2181		## Use localhost in case of single node 
	$ telnet localhost 2181			## Use localhost in case of single node 
	$ jps
		<processid> QuorumPeerMain
	$ sh bin/kafka-server-start.sh -daemoan config/server.properties			## Using -daemoan to start the service in background
	
	## Validation for kafka server 
	$ telnet <ipaddress> 9092 		## Use localhost in case of single node 
	$ telnet localhost 9092			## 
	
	$ jps
		<processid> QuorumPeerMain
		<processid> Kafka
		
3. Create & list a topic:
	$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
	$ bin/kafka-topics.sh --list --zookeeper localhost:2181
	$ bin/kafka-topics.sh --list --zookeeper localhost:2181 --topic test 		## To list a particular topic 	

4. Producer data to the above topic & Consume it
	## Syntax: kafka-console-producer.sh will use "-broker-list" 
	## Syntax: kafka-console-consumer.sh will use "--bootstrap-server"		
	## Both broker-list and bootstrap-server are same but don't know why they have different argument name for producer and consumer
	$ bin/kafka-console-producer.sh  --broker-list localhost:9092 --topic test 
	$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
	
5. Delete the topic
	$ bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic test
	## The above command is used to set Topic is marked for deletion  
	delete.topic.enable true
	
6. To pump the output of tail command to the kafka topic test
	$ tail -F access.log | kafka-console-producer.sh  --broker-list localhost:9092 --topic test 	
	
	
## Note: About bootstrap-server and port 9092, dataDir, logDir, etc 
	## Option 1
	$ cd /kafka/kafka_2.11-2.1.0
	$ cat server.properties | grep 9092
		#     listeners = PLAINTEXT://your.host.name:9092
		#listeners=PLAINTEXT://:9092
		#advertised.listeners=PLAINTEXT://your.host.name:9092
	
	## Option 2
	a. Go to Cloudera Manager or Ambari and open the Kafka broker, it will show the listers and zookeeper.connect which is bootstrap-server 
	b. Get the port number from listener. It is 6667 in ITVersity
	c. 
	
##################################################################################
Kafka - Initial setup 
##################################################################################
Ref: ITVersity: https://www.youtube.com/watch?v=8w7jQ601HJk&list=PLf0swTFhTI8rM5AdWXObgQ-aN5gYyOfKC&index=2

## Pre-request:
	a. Check the Version compatibility of Kafka, Spark Streaming, target (Hbase, bigdata, etc) 
	b. Go to Kafka library path in hadoop -> (Ex: In horton works) -> "cd /usr/hdp/2.6.5.0-292/kafka/libs" -> kafka jar will be here
	c. kafka_2.11-1.0.0.2.6.5.0-292-source.jar.asc  ## Below are compatible version# 
		a. 2.11 is for scala
		b. 1.0.0 is for Kafka 
		c. 2.6.5.0 is for Hadooop 
		d. Note: Scala 2.11 is not compatiable with JDK 1.9, so use JDK 1.8
		e. For SBT, use 0.13.17 
		
1. download & untar
	$ wget https://www-eu.apache.org/dist/kafka/2.1.0/kafka_2.11-2.1.0.tgz
	$ tar -xzf kafka_2.11-2.1.0.tgz
	
2. SBT: IntelliJ - Choose the correct scala version and set the SBT as follows:
	name := "MyKafkaProject"
	version := "1.0"
	scalaVersion := "2.11.12"
	
	## To pass the broker information as part of our program, include the below library as well (it is compatiable with most of the scala version).. Hardcoding is not good practice. 
	libraryDependencies += "com.typesafe" % "config" % "1.3.2"
	
	## To include the scala and kafka libraries (we can use %% to get the scala version from above, but it is not required for kafka-clients ) 
	libraryDependencies += "org.apache.kafka" % "kafka-clients" % "1.0.0"

	NOTE: ## Finally click on "enable auto import" button in IntelliJ to import all the above libraries


4. Maven: Dependency for Producer and Consumer API 

	<dependency> 
		<groupId>org.apache.kafka</groupId>
		<artifactId>kafka-clients</artifactId>
		<version>0.10.1.0</version>
	</dependency> 
	
3. The above step will add the external libraries for the following packages	
	a. org.apache.kafka ## This is a package name 
		i. It has producer/consumer libraries
		ii. It also has common library which can be used for 'serialization'/'deserialization'
	b. com.typesafe
	
##########################################################################
Kafka - Execution using shell 
##########################################################################

# Below kafka shells are just for testing. But in real-time schenario, we will never use shells
# All the below syntax will vary depends upon the kafka version. The below syntax is for kafka 0.8
# Partition is for scalability and replication factor is for high availability
# Specify ZK host with port 2181
kafka-topics.sh --create \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --replication-factor 1 \
  --partitions 1 \
  --topic kafka-saranvisa-topic1

kafka-topics.sh --list \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --topic kafka-saranvisa-topic1

# Command to produce messages, start typing after running this kakfa-console-producer command
# The messages will be stored in topic kafkadg on the host where brokers are running
# Specify all the Kafka broker host with port 6667 , 9092
kafka-console-producer.sh \
  --broker-list nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667 \
  --topic kafka-saranvisa-topic1


# Open another shell and then run kafka-console-consumer command to see streaming messages
# Make sure to setup KAFKA_HOME, PATH in new shell
# from-beginning means from offset 0, from-now means only current value
kafka-console-consumer.sh \
  --bootstrap-server nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667 \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --topic kafka-saranvisa-topic1 \
  --from-beginning


## To describe the topic
kafka-topics.sh --describe \
--zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
--topic kafka-saranvisa-topic1


##################################################################################
SBT: To install SBT (Scala built tool)
##################################################################################

https://www.youtube.com/watch?v=k8XhuE1PE7A&list=PLkz1SCf5iB4enAR00Z46JwY9GGkaS2NON&index=9

1. Pre-request: Java
2. Install the sbt compatiable to windows or unix as needed 
	https://www.scala-sbt.org/download.html
	
3. After the installation, create the below two files: https://www.learningjournal.guru/courses/kafka/kafka-foundation-training/producer-api/

	##1: vi build.sbt 
	name := "KafkaTest"

    libraryDependencies ++= Seq(
        "org.apache.kafka" % "kafka-clients" % "0.10.1.0"
        exclude("javax.jms", "jms")
        exclude("com.sun.jdmk", "jmxtools")
        exclude("com.sun.jmx", "jmxri")
        exclude("org.slf4j", "slf4j-simple")
    )

	##2: vi SimpleProducer.java
	 import java.util.*;
     import org.apache.kafka.clients.producer.*;                                    
     public class SimpleProducer {                                    
         public static void main(String[] args) throws Exception{

            String key = "Key1";
            String value = "Value-1";
            String topicName = "SimpleProducerTopic";
                                    
            Properties props = new Properties();
            props.put("bootstrap.servers", "localhost:9092,localhost:9093");
            props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");         
            props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
                                    
            Producer<String, String> producer = new KafkaProducer<>(props);                                    
            ProducerRecord<String, String> record = new ProducerRecord<>(topicName,key,value);
            producer.send(record);                
            producer.close();
            System.out.println("SimpleProducer Completed.");
        }
    }

	$ sbt compile	## To compile the above program 
	$ sbt run		## To execute the program 
	
	
	##3: Here is a simple example of using the producer to send records with strings containing sequential numbers as the key/value pairs.
	 Properties props = new Properties();
	 props.put("bootstrap.servers", "localhost:9092");
	 props.put("acks", "all");
	 props.put("retries", 0);
	 props.put("batch.size", 16384);
	 props.put("linger.ms", 1);
	 props.put("buffer.memory", 33554432);
	 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
	 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

	 Producer<String, String> producer = new KafkaProducer<>(props);
	 for (int i = 0; i < 100; i++)
		 producer.send(new ProducerRecord<String, String>("my-topic", Integer.toString(i), Integer.toString(i)));

	 producer.close();

##################################################################################
Kafka Configuration: Kafka uses key-value pairs in the property file format for configuration
##################################################################################
https://kafka.apache.org/documentation/#configuration

3.1 Broker Configs
3.2 Topic Configs
3.3 Producer Configs
3.4 Consumer Configs
3.5 Kafka Connect Configs
3.6 Kafka Streams Configs
3.7 AdminClient Configs

3.1 Broker Configs
	The essential configurations are the following:
		a. broker.id
		b. log.dirs
		c. zookeeper.connect
	
	Other important configs 
		d. log.retention.ms 	(Default is time and 7 days)
		e. log.retention.bytes 	(This is not a topic size, it is a size of partition)


##################################################################################
Kafka-ansible-roles-playbook 	-- Pending 
##################################################################################

https://github.com/dgadiraju/kafka-ansible-roles-playbook

##################################################################################
Kafka - Streaming 				-- Pending 
##################################################################################
https://www.youtube.com/watch?v=NhDM9dFnTA8&feature=push-lsb&attr_tag=ET_GAXm0_u_nZJHP%3A6

###################################################################################
Streaming Pipelines - Kafka Core Concepts and Overview Of Logstash *** Elasticsearch
##################################################################################

ITVersity(Logstash) : https://www.youtube.com/watch?v=NhDM9dFnTA8






