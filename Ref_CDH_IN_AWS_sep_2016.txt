======================= Reference for cloudera cluster 'auto' setup using python ============
https://github.com/cloudera/cm_api/tree/master/python/examples/auto-deploy
1. Make sure to meet the pre-request
2. Read the README.md to understand how to execute the script
3. Test to validate the installation 

======================= Cloudera API ==========================================================
Usecase: 
1. Automated deployment of new clusters, using a combination of Puppet and the Cloudera Manager API. Puppet does the OS-level provisioning and installs the software. The Cloudera Manager API sets up the Hadoop services and configures the cluster. Use the web link below
The API also provides access to management functions:
2.	Obtaining logs and monitoring the system
3.	Starting and stopping services
4.	Polling cluster events
5.	Creating a disaster recovery replication schedule

https://www.cloudera.com/documentation/enterprise/5-9-x/topics/cm_intro_automation_api.html#concept_hlz_m5x_fz

======================= Cloudera cluster 'manual' setup =========================================

-------- Using Ubuntu in AWS
http://gethue.com/hadoop-tutorial-how-to-create-a-real-hadoop-cluster-in-10-minutes/


----Before installation
1. Finalize the RedHat version  										** RedHat 6.5 **
2. Identify the corresponding MySQL version suitable for Redhat    		** mysql57-community-release-el6-9.noarch.rpm **
3. Identify the corresponding CDH version suitable for MySQL			** CDH 5.7.0 **
4. Finalize the Linux File System-Mounting Point and make sure Separate Mounting Point for /var, so that MySQL will refer to /var instead of root(/)
   NOTE: Cloudera navigator will occupy more space in MySQL (after couple of years), so if MySQL configured in root, it will create trouble
5. Finalize the Linux File System-Mounting Point and make sure DataNode is NOT referring to ROOT
Cloudera Manager -> HDFS -> Configuration -> DataNode Data Directory (dfs.data.dir, dfs.datanode.data.dir)
Ex: File System
$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/md2        459G   73G  363G  17% /
tmpfs            63G  4.0K   63G   1% /dev/shm
/dev/md0        184M  121M   55M  69% /boot			## It is good to have separate space for Linux Kernal
/dev/sda4       6.6T  107G  6.5T   2% /u01
/dev/sdb4       6.6T  978M  6.6T   1% /u02
/dev/sdc1       7.1T  329G  6.8T   5% /u03
/dev/sdd1       7.1T  311G  6.8T   5% /u04
/dev/sde1       7.1T  333G  6.8T   5% /u05
/dev/sdf1       7.1T  341G  6.8T   5% /u06
/dev/sdg1       7.1T  345G  6.8T   5% /u07
/dev/sdh1       7.1T  327G  6.8T   5% /u08
/dev/sdi1       7.1T  330G  6.8T   5% /u09
/dev/sdj1       7.1T  331G  6.8T   5% /u10
/dev/sdk1       7.1T  331G  6.8T   5% /u11
/dev/sdl1       7.1T  328G  6.8T   5% /u12
cm_processes     63G   15G   49G  24% /var/run/cloudera-scm-agent/process

Cloudera Manager -> HDFS -> Configuration -> DataNode Data Directory (dfs.data.dir, dfs.datanode.data.dir)
# DataNode Default Group
/u12/hadoop/dfs,
/u11/hadoop/dfs,
/u10/hadoop/dfs,
/u09/hadoop/dfs,
/u08/hadoop/dfs,
/u07/hadoop/dfs,
/u06/hadoop/dfs,
/u05/hadoop/dfs,
/u04/hadoop/dfs,
/u03/hadoop/dfs,
/u02/hadoop/dfs,
/u01/hadoop/dfs

EX: $ ls -ltr /u04/hadoop/dfs/current
total 8
-rw-r--r-- 1 hdfs hadoop  197 Mar 24 15:13 VERSION
drwx------ 4 hdfs hadoop 4096 Mar 24 15:13 BP-241413917-10.3.2.1-1458616808423

# DataNode : Using this option we can also customize the file system for DataNode differently for each Nodes

6. Finalize the Linux File System-Mounting Point and make sure YARN NodeManager is NOT referring to ROOT
Cloudera Manager -> YARN -> Configuration -> NodeManager Local Directories (yarn.nodemanager.local-dirs)

# NodeManager Default Group
/u12/hadoop/yarn/nm,
/u11/hadoop/yarn/nm,
/u10/hadoop/yarn/nm,
/u09/hadoop/yarn/nm,
/u08/hadoop/yarn/nm,
/u07/hadoop/yarn/nm,
/u06/hadoop/yarn/nm,
/u05/hadoop/yarn/nm,
/u04/hadoop/yarn/nm,
/u03/hadoop/yarn/nm,
/u02/hadoop/yarn/nm,
/u01/hadoop/yarn/nm

7. Finalize the Linux File System-Mounting Point and make sure Impala Daemon is NOT referring to ROOT
Cloudera Manager -> Impala -> Configuration -> Impala Daemon Scratch Directories (scratch_dirs)

# Impala Daemon Default Group
/u01/impala/impalad,
/u02/impala/impalad,
/u03/impala/impalad,
/u04/impala/impalad,
/u05/impala/impalad,
/u06/impala/impalad,
/u07/impala/impalad,
/u08/impala/impalad,
/u09/impala/impalad,
/u10/impala/impalad,
/u11/impala/impalad,
/u12/impala/impalad

[root@bda1node03 impalad]# pwd
/u01/impala/impalad/impala-scratch

By default, intermediate files used during large sort, join, aggregation, or analytic function operations are stored in the directory /tmp/impala-scratch . These files are removed when the operation finishes. You can specify a different location by starting the impalad daemon with the --scratch_dirs="path_to_directory" configuration option. The scratch directories must be on the local filesystem, not in HDFS.

As on 2015, Impala does not support limiting scratch file usage. You can achieve this at the operating system level with disk quotas for the impala user or by putting the scratch directory on a separate disk partition.

https://www.cloudera.com/documentation/enterprise/5-9-x/topics/impala_disk_space.html
https://community.cloudera.com/t5/Interactive-Short-cycle-SQL/Impala-scratch-file/m-p/31907#M1085


8. How to handle Memory overcommit issue between Yarn & Impala? Pending

------------
First time login
ssh ec2-user@ec2-34-123-123-123.compute-1.amazonaws.com

------------ set password for root
sudo passwd root;

## For New users
# hadoop1
groupadd hadoop; useradd kumar -g hadoop; usermod -aG wheel  kumar; passwd kumar;
# pwd: hadoop2


## For existing users
$ groupadd hive-users
$ usermod -G someuser,hive-users someuser
$ usermod -G hive,hive-users hive


# hadoop2
chmod 600 /etc/sudoers
vi /etc/sudoers 

--------------Redhat 6.5
su root
yum localinstall https://dev.mysql.com/get/mysql57-community-release-el6-9.noarch.rpm
sudo yum install mysql-server
sudo yum install mysql-connector-java  # This will install the old version of mysql-connector. Follow the below step to replace only .jar with latest version

#---mysql-connector-java # follow the below steps
Copying mysql-connector-java.xxx to /usr/share/java/ has fixed the issue. Follow the below steps:
 
1. Get the latest (or) suitable mysql-connector from the below link
http://dev.mysql.com/downloads/connector/j/5.1.html
wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.40.tar.gz
 
2. tar zxvf mysql-connector-java-5.1.40.tar.gz
 
3. sudo cp /home/kumar/mysql/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar      /usr/share/java/
 
4. If /usr/share/java path already has a link mysql-connector-java.jar and it is referring to any old mysql-connector. Ex: mysql-connector-java-5.1.31-bin.jar then remove the link mysql-connector-java.jar and make it refer to the newly copied mysql-connector-java-5.1.40-bin.jar
 
rm mysql-connector-java.jar
ln -s /usr/share/java/mysql-connector-java-5.1.40-bin.jar mysql-connector-java.jar
## Expected result: mysql-connector-java.jar -> /usr/share/java/mysql-connector-java-5.1.40-bin.jar
 
5. Restart your hive service in CM
 
6. For more details, refer http://www.cloudera.com/documentation/enterprise/5-7-x/topics/cm_ig_mysql.html
 
#---mysql-connector-java #end

sudo service mysqld status;sudo service mysqld start;sudo service mysqld status;
sudo /etc/init.d/mysqld stop
sudo mysqld_safe --skip-grant-tables &
mysql -u root
update mysql.user set password=PASSWORD('hadoop1') where User='root';  
(or) -- if the above command throws any error run the below update command
update mysql.user set authentication_string=password('hadoop1') where user='root';
flush privileges;
sudo /etc/init.d/mysqld start
mysql -u root -p

ALTER USER 'root'@'localhost' IDENTIFIED BY 'hadoop1';
create database hive;create user 'hive' identified by 'hadoop1'; grant all on hive.* to hive; flush privileges;
create database rman;create user 'rman' identified by 'hadoop1'; grant all on rman.* to rman; flush privileges;
create database oozie_server;create user 'oozie_server' identified by 'hadoop1'; grant all on oozie_server.* to oozie_server; flush privileges;
create database hue;create user 'hue' identified by 'hadoop1'; grant all on hue.* to hue; flush privileges;
create database amon;create user 'amon' identified by 'hadoop1'; grant all on amon.* to amon; flush privileges;
create database sentry;create user 'sentry' identified by 'Hadoop1@'; grant all on sentry.* to sentry; flush privileges;
exit;
mysql -u hive -p



------------Disable SELINUX, Firewall 
vi  /etc/selinux/config

service ip6tables status;service ip6tables stop;service ip6tables status;
service ip6tables status;service ip6tables stop;service ip6tables status;
chkconfig --list iptables; chkconfig iptables off; chkconfig --list iptables; 
chkconfig --list ip6tables; chkconfig ip6tables off; chkconfig --list ip6tables; 


------------Update hostname
vi /etc/sysconfig/network
HOSTNAME=<identify the public host and assign>

vi /etc/hosts
10.0.0.203 ec2-34-123-123-123.compute-1.amazonaws.com
10.0.0.62 ec2-34-124-124-124.compute-1.amazonaws.com

------------ Enable password authentication
vi /etc/ssh/sshd_config
PasswordAuthentication yes
# PasswordAuthentication No
service sshd restart

--------- restart the machine
init 6

------------Verifiy ssh after enable password authentication
ssh kumar@ec2-34-123-123-123.compute-1.amazonaws.com
ssh kumar@ec2-34-124-124-124.compute-1.amazonaws.com


--------- Enable password less login
# Master node
ssh-keygen  

# Slave node
pwd # make sure it is /home/username
mkdir -p .ssh

#master node
cat ~/.ssh/id_rsa.pub | ssh kumar@ec2-34-123-123-123.compute-1.amazonaws.com "cat >> ~/.ssh/authorized_keys"

# Slave node
chmod 600 authorized_keys


----------CDH Installation
mkdir cm5
cd cm5
http://archive.cloudera.com/cm5/
wget http://archive.cloudera.com/cm5/installer/5.7.0/cloudera-manager-installer.bin
chmod +x   cloudera-manager-installer.bin

sudo ./cloudera-manager-installer.bin



Impala 1.4.0 (or higher) running with strong authentication. With Impala, either Kerberos or LDAP can be configured to achieve strong authentication

################## Test to validate the installation ###############

https://github.com/cloudera/cm_api/tree/master/python/examples/auto-deploy#tests-to-validate-the-installation

#Run these on one of the datanodes in the cluster

#Create hive table
hive shell
hive> CREATE TABLE justin(id INT, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;
hive> exit;

#Create HDFS file
echo "1 justin" > /tmp/test
sudo -u hdfs hadoop fs -put /tmp/test /user/hive/warehouse/justin/

#Query hive table
hive shell
hive> select * from justin where id=1;
hive> exit;

#Query same table but in impala
impala-shell
impala> invalidate metadata;
impala> select * from justin;
impala> exit;

#Run mapreduce job
hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-examples.jar pi 10 10

#Test hbase
hbase shell
hbase> create 'test', 'cf'
hbase> list 'test'
hbase> put 'test', 'row1', 'cf:a', 'value1'
hbase> scan 'test'
hbase> exit

#Test spark - replace $NAMENODE with the correct hostname that's running the NN
echo "this is the end. the only end. my friend." > /tmp/sparkin
hadoop fs -put /tmp/sparkin /tmp/
spark-shell
scala> val file = sc.textFile("hdfs://$NAMENODE:8020/tmp/sparkin")
scala> val counts = file.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
scala> counts.saveAsTextFile("hdfs://$NAMENODE:8020/tmp/sparkout")
scala> exit
hadoop fs -cat /tmp/sparkout

#########################################################
#### KERBEROS INSTALLATION & CONFIGURUATION ####
#########################################################
#### PRE-REQUEST AES-256 ####

######### Verygood reference for Kerberos installation ############
1. https://www.youtube.com/watch?v=4TwU0LwDJAg
2. https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.1.0/authentication-with-kerberos/content/kerberos_optional_install_a_new_mit_kdc.html

Steps:
-----
 Install MIT Kerberos
	a. To install MIT Kerbros on "RHEL/CentOS/Oracle Linux"
			$ yum install krb5-server krb5-libs krb5-workstation
	b. To install MIT Kerbros on "Ubuntu/Debian"
			$ apt-get install krb5-kdc krb5-admin-server
	c. Two conf files
		Vi /etc/krb5.conf
		vi /var/kerberos/krb5kdc/kdc.conf

	d. Edit "/etc/krb5.conf"
		"Change the [realms] section of this file by replacing the default “kerberos.example.com” setting for the kdc and admin_server properties with the Fully Qualified Domain Name of the KDC server host"
		
		[libdefaults]
		 default_realm = AWS.COM
		 dns_lookup_realm = false
		 dns_lookup_kdc = false
		 ticket_lifetime = 24h
		 renew_lifetime = 7d
		 forwardable = true
		 default_tgs_enctypes = aes256-cts
		 default_tkt_enctypes = aes256-cts
		 permitted_enctypes = aes256-cts
		 udp_preference_limit = 1

		[realms]
		 MYAWS.COM = {
		  kdc = ec2-34-192-193-57.compute-1.amazonaws.com:88
		  admin_server = ec2-34-192-193-57.compute-1.amazonaws.com:749
		 }

		[domain_realm]
		 .compute-1.amazonaws.com = AWS.COM
		 compute-1.amazonaws.com = AWS.COM


	e. Use the utility kdb5_util to create the Kerberos database:
		1. On "RHEL/CentOS/Oracle Linux":	$ kdb5_util create -s
		2. On "Ubuntu/Debian":				$ krb5_newrealm
		
	f. Start the KDC server and the KDC admin server:
		1. On "RHEL/CentOS/Oracle Linux 6"	$ /etc/rc.d/init.d/krb5kdc start  , $ /etc/rc.d/init.d/kadmin start
		2. On "RHEL/CentOS/Oracle Linux 7"  $ systemctl start krb5kdc, 			$ systemctl start kadmin
		3. On "Ubuntu/Debian":			$ service krb5-kdc restart, 		$ service krb5-admin-server restart

	g. Set up the KDC server to auto-start on boot:
		1. On "RHEL/CentOS/Oracle Linux 6" 	$ chkconfig krb5kdc on, 			$ chkconfig kadmin on
		2. On "RHEL/CentOS/Oracle Linux 7"	$ systemctl enable krb5kdc,			$ systemctl enable kadmin
		3. On "Ubuntu/Debian"		$ update-rc.d krb5-kdc defaults		$ update-rc.d krb5-admin-server defaults
		
	h. Create a Kerberos Admin (or) RHEL Edit "vi /var/kerberos/krb5kdc/kdc.conf" (or) Ubuntu Edit "vi /etc/krb5kdc/kadm5.acl"
		Kerberos principals can be created either on the KDC machine itself or through the network, using an “admin” principal. The following instructions assume you are using the KDC machine and using the kadmin.local command line administration utility. Using kadmin.local on the KDC machine allows you to create principals without needing to create a separate "admin" principal before you start.
		
		a. Create a KDC admin by creating an admin principal: kadmin.local -q "addprinc admin/admin".
		b. Confirm that this admin principal has permissions in the KDC ACL. Using a text editor, open the KDC ACL file:
			RHEL Edit "vi /var/kerberos/krb5kdc/kdc.conf" (or) Ubuntu Edit "vi /etc/krb5kdc/kadm5.acl"
			[realms]
				 AWS.COM = {
				  master_key_type = aes256-cts
				  max_life = 180d 0h 0m 0s
				  max_renewable_life = 7d 0h 0m 0s
				  acl_file = /var/kerberos/krb5kdc/kadm5.acl
				  dict_file = /usr/share/dict/words
				  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
				  #supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
				  supported_enctypes = aes256-cts:normal
				  default_principal_flags = +renewable
				 }
		c. Ensure that the KDC ACL file includes an entry so to allow the admin principal to administer the KDC for your specific realm. When using a realm that is different than EXAMPLE.COM, be sure there is an entry for the realm you are using. If not present, principal creation will fail. For example, for an admin/admin@HADOOP.COM principal, you should have an entry: */admin@HADOOP.COM *.
		d. After editing and saving the kadm5.acl file, you must restart the kadmin process:
			1. On "RHEL/CentOS/Oracle Linux 6"	$ /etc/rc.d/init.d/krb5kdc start  , $ /etc/rc.d/init.d/kadmin start
			2. On "RHEL/CentOS/Oracle Linux 7"  $ systemctl start krb5kdc, 			$ systemctl start kadmin
			3. On "Ubuntu/Debian":		$ service krb5-kdc restart, 		$ service krb5-admin-server restart

#################

# How to check we have AES-256 in our environment?
# wget the file is getting some trouble, so use winscp
# Get the java file windows: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html
# winscp to linux path "/home/kumar/jdk"
# The following path might have old files, just replace it : /usr/java/jdk1.7***/jre/lib/security 

cp /home/kumar/jdk/jce_policy-8/UnlimitedJCEPolicyJDK8/local_policy.jar /usr/java/jdk1.7.0_67-cloudera/jre/lib/security
cp /home/kumar/jdk/jce_policy-8/UnlimitedJCEPolicyJDK8/US_export_policy.jar /usr/java/jdk1.7.0_67-cloudera/jre/lib/security
cp /home/kumar/jdk/jce_policy-8/UnlimitedJCEPolicyJDK8/US_export_policy.jar /usr/java/jdk1.6.0_31/jre/lib/security
cp /home/kumar/jdk/jce_policy-8/UnlimitedJCEPolicyJDK8/local_policy.jar /usr/java/jdk1.6.0_31/jre/lib/security

#########################################################
#### INSTALL KERBEROS ####
https://github.com/daisukebe/krb-bootstrap/blob/master/configure_kdc_with_aes256_before_cm-wizard.txt

# Install on MASTER NODE
yum install krb5-server  
yum install openldap-clients 
yum install krb5-workstation, krb5-libs 

## Install on ALL OTHER NODES
yum install krb5-workstation, krb5-libs 


#########################################################
#### CONFIGURATION CHANGE ####

2. Edit Configuration files
# Edit following files
vi /etc/krb5.conf

# apply the following command
:%s/example.com/AWS.COM
:%s/EXAMPLE.COM/AWS.COM

# Add the highlighted last 4 lines under libdefaults (for aes256-cts)
[libdefaults]
 default_realm = AWS.COM
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 default_tgs_enctypes = aes256-cts
 default_tkt_enctypes = aes256-cts
 permitted_enctypes = aes256-cts
 udp_preference_limit = 1

# Make sure kdc and admin_server assigned to Public IP as mentioned below
[realms]
 AWS.COM = {
  kdc = ec2-34-123-123-123.compute-1.amazonaws.com:88
  admin_server = ec2-124-124-124.compute-1.amazonaws.com:749
 }

[domain_realm]
 .compute-1.amazonaws.com = AWS.COM
 compute-1.amazonaws.com = AWS.COM
 
 ---------
 /etc/krb5.conf
 chown cloudera-scm:cloudera-scm /etc/krb5.conf
 chmod 600 /etc/krb5.conf

#########################################################
/var/kerberos/krb5kdc/kdc.conf

#After Change
[realms]
 AWS.COM = {
  master_key_type = aes256-ctsi
  max_life = 180d 0h 0m 0s
  max_renewable_life = 7d 0h 0m 0s
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  #supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
  supported_enctypes = aes256-cts:normal
  default_principal_flags = +renewable
 }

#########################################################
/var/Kerberos/krb5kdc/kadm5.acl
Replace EXAMPLE.COM with AWS.COM
*/admin@AWS.COM *

----------
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm kadm5.acl
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm principal.kadm5
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm principal.kadm5.lock
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm principal.ok
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm principal
[root@ec2-34-123-123-123 krb5kdc]# chown cloudera-scm:cloudera-scm kdc.conf

#########################################################
#### KERBEROS DB CREATION ####
# Create Kerberos Database using kdb5_util (remember password hadoop1)
# NOTE: if the command gets hung with 'Loading random data', see http://championofcyrodiil.blogspot.in/2014/01/increasing-entropy-in-vm-for-kerberos.html

kdb5_util create –s –r AWS.COM
	
#########################################################	
#### START THE SERVICE ####

service kadmin start;service kadmin status;chkconfig kadmin on;chkconfig --list kadmin
service krb5kdc start;service krb5kdc status;chkconfig krb5kdc on;chkconfig --list krb5kdc

######################################################### 
#### CREATE PRINCIPAL ####
http://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s3_cm_principal.html
# Get or Create a Kerberos Principal for the Cloudera Manager Server
# The Cloudera Manager Server must have the correct Kerberos principal that has privileges to create other accounts.

>kadmin.local
addprinc -randkey -pw hadoop1 cloudera-scm/admin@AWS.COM
addprinc -randkey -pw hadoop1 root/admin@AWS.COM
addprinc -randkey -pw hadoop1 hdfs/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
addprinc -randkey -pw hadoop1 mapred/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
addprinc -randkey -pw hadoop1 yarn/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
addprinc -randkey -pw hadoop1 hive/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
addprinc -randkey -pw hadoop1 HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
addprinc -randkey -pw hadoop1 solr/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM    ## Check the Public IP before add the principal
addprinc -pw hadoop1 hdfs@AWS.COM
addprinc -pw hadoop1 hive@AWS.COM
addprinc -pw hadoop1 root@AWS.COM
addprinc -pw hadoop1 kumar@AWS.COM
addprinc -pw hadoop1 kumar1@AWS.COM
#########################################################

# Add only child hosts
addprinc -pw hadoop1 host/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
 
######################################################### 
#### ON HOLD #### Go to next step ##

## ON HOLD # Not needed # Add HTTP, httpfs, hdfs
# addprinc -randkey -pw hadoop1 HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 httpfs/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 hdfs/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 mapred/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 yarn/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

## ON HOLD # Just for reference
# To Add all the services **** DO NOT ADD THE BELOW SERVICES TO ENABLE KERBEROS USING wizard *** OTHERWISE IT WILL THROW BELOW ERROR
# add_principal: Principal or policy already exists while creating "oozie/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM"
# For Hadoop, the principals should be of the format username/fully.qualified.domain.name@YOUR-REALM.COM
# username of an existing Unix account, such as hdfs or mapred
# Note: Not all of them are valid username from "cat /etc/passwd | grep login". But it will help to identify the available username
# Syntax: addprinc -randkey hdfs/fully.qualified.domain.name@YOUR-REALM.COM
## To delete the existing principal: delprinc HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

## ON HOLD # Just for reference
# addprinc -randkey -pw hadoop1 hive/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 hue/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 mapred/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 yarn/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 zookeeper/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
# addprinc -randkey -pw hadoop1 oozie/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM
*** Pending *** (sqoop is missing in K12)
# sentry:x:488:487:Sentry:/var/lib/sentry:/sbin/nologin
# spark:x:486:485:Spark:/var/lib/spark:/sbin/nologin
# sqoop:x:485:484:Sqoop:/var/lib/sqoop:/sbin/nologin

######################################################### 
#### CREATE KEYTAB #####

# Get or Create a Kerberos Principal and Keytab File for the Cloudera Manager Server
http://www.cloudera.com/documentation/archive/manager/4-x/4-5-1/Configuring-Hadoop-Security-with-Cloudera-Manager/cmeechs_topic_4_5.html
https://www.cloudera.com/documentation/enterprise/5-3-x/topics/cdh_sg_kerberos_prin_keytab_deploy.html

# Create the Cloudera Manager Server cmf.keytab file:
# NOTE: The Cloudera Manager Server keytab file must be named cmf.keytab because that name is hard-coded in Cloudera Manager.
kadmin.local:
 xst -norandkey -k /home/kumar/cmf.keytab cloudera-scm/admin@AWS.COM

# Create the hdfs.keytab files:
kadmin.local: 
  xst -norandkey -k /home/kumar/hdfs.keytab hdfs/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

# Create the mapred.keytab files:
kadmin.local: 
  xst -norandkey -k /home/kumar/mapred.keytab mapred/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

# Create the yarn.keytab files:
kadmin.local: 
  xst -norandkey -k /home/kumar/yarn.keytab yarn/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

# Create the hive.keytab files:
kadmin.local: 
  xst -norandkey -k /home/kumar/hive.keytab yarn/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM HTTP/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM

#########################################################
#### DISPLAY THE KEYTAB #### 

# Use klist to display the keytab file entries
$
klist -e -k -t /home/kumar/cmf.keytab
klist -e -k -t /home/kumar/hdfs.keytab
klist -e -k -t /home/kumar/mapred.keytab
klist -e -k -t /home/kumar/yarn.keytab
klist -e -k -t /home/kumar/hive.keytab

# Use the ktutil to display the keytab as follows
ktutil: read_kt hdfs.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/cmf.keytab
ktutil: read_kt hdfs.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/hdfs.keytab
ktutil: wkt /etc/hadoop/conf/hdfs.keytab

ktutil: read_kt mapred.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/mapred.keytab
ktutil: wkt /etc/hadoop/conf/mapred.keytab

ktutil: read_kt yarn.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/yarn.keytab
ktutil: wkt /etc/hadoop/conf/yarn.keytab

ktutil: read_kt hive.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/hive.keytab
ktutil: wkt /etc/hadoop/conf/hive.keytab

cd etc/cloudera-scm-server
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/cmf.keytab 
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/hdfs.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/mapred.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/yarn.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/hive.keytab
sudo chmod 600 /etc/cloudera-scm-server/*.keytab

cd /etc/hadoop/conf
sudo chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab
sudo chown mapred:hadoop /etc/hadoop/conf/mapred.keytab
sudo chown yarn:hadoop /etc/hadoop/conf/yarn.keytab
sudo chown hive:hadoop /etc/hadoop/conf/hive.keytab
sudo chmod 400 /etc/hadoop/conf/*.keytab


######################################################### --old
#### DEPLOY THE KEYTAB ####

## Deploying the Cloudera Manager Server Keytab
http://www.cloudera.com/documentation/archive/manager/4-x/4-5-1/Configuring-Hadoop-Security-with-Cloudera-Manager/cmeechs_topic_4_6.html

# Move the cmf.keytab file to the /etc/cloudera-scm-server/ directory on the host machine where you are running the Cloudera Manager Server.
mv cmf.keytab hdfs.keytab mapred.keytab yarn.keytab hive.keytab /etc/cloudera-scm-server/


(or) use the kutil to move cmf.keytab
# Use the ktutil to display the keytab as follows
ktutil: read_kt hdfs.keytab   
ktutil: list
ktutil: wkt /etc/cloudera-scm-server/cmf.keytab


# Make sure that the cmf.keytab file is only readable by the Cloudera Manager Server user account cloudera-scm.
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/cmf.keytab 
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/hdfs.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/mapred.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/yarn.keytab
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/hive.keytab
sudo chmod 600 /etc/cloudera-scm-server/*.keytab

# Add the Cloudera Manager Server principal (cloudera-scm/admin@AWS.COM) to a text file named cmf.principal and store the cmf.principal file in the 
# /etc/cloudera-scm-server/ directory on the host machine where you are running the Cloudera Manager Server
# Generate "cmf.principal.tmp" do all the changes then change the name to "cmf.principal" as follows
vi /etc/cloudera-scm-server/cmf.principal.tmp
cloudera-scm/admin@AWS.COM
sudo chmod 0600 /etc/cloudera-scm-server/cmf.principal.tmp
sudo chown cloudera-scm:cloudera-scm /etc/cloudera-scm-server/cmf.principal.tmp
mv /etc/cloudera-scm-server/cmf.principal.tmp /etc/cloudera-scm-server/cmf.principal
# As needed you can kinit to cloudera-scm using below command
kinit -k -t /etc/cloudera-scm-server/cmf.keytab cloudera-scm/admin@AWS.COM

## On every node in the cluster, repeat the following steps to deploy the hdfs.keytab, mapred.keytab and yarn.keytab files
# On each respective machine, the HTTP principal in the yarn keytab file will be the same as the HTTP principal in the hdfs and mapred keytab files.
# Use mv command or ktutil as follows
sudo mv hdfs.keytab mapred.keytab yarn.keytab hive.keytab /etc/hadoop/conf/
(or) 
ktutil
Ktutil: list
Ktutil: rkt /home/kumar/hive.keytab 
Kttuil: list
ktutil: wkt /etc/hadoop/conf/hive.keytab

# Make sure that the hdfs.keytab, mapred.keytab, yarn.keytab, hive.keytab files are only readable by the hdfs, mapred, yarn, hive users respectively
sudo chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab
sudo chown mapred:hadoop /etc/hadoop/conf/mapred.keytab
sudo chown yarn:hadoop /etc/hadoop/conf/yarn.keytab
sudo chown hive:hadoop /etc/hadoop/conf/hive.keytab
sudo chmod 400 /etc/hadoop/conf/*.keytab

#########################################################
#### ENABLE KERBEROS USING WIZARD ####

#########################################################
#### SETUP REALM in CM ####

CM -> Administration -> Setting -> Kerberos -> Kerberos Security REALM = AWS.COM
CM -> Administration -> Setting -> Kerberos -> KDC SERVER HOST = ec2-34-123-123-123.compute-1.amazonaws.com
CM -> Administration -> Setting -> Kerberos -> Encryption type = aes256-cts

#########################################################
#### STOP ALL SERVICES ####
# Before you enable security in CDH, you must stop all Hadoop daemons in your cluster and then change some configuration properties

CM -> Cluster -> STOP

CM -> Administration -> Setting -> Import KDC Account Manager Credentials

#  case of failure, do the below change in /usr/share/cmf/bin/import_credentials.sh
KRB5_CONFIG=/etc/cloudera-scm-server/cmf.keytab

Enabling Kerberos

#########################################################
#### Create the HDFS Superuser ####

# To be able to create home directories for users, you will need access to the HDFS superuser account

1. Go to the Cloudera Manager Admin Console and navigate to the HDFS service.
2. Click the Configuration tab.
3. Select Scope > HDFS (Service-Wide).
4. Select Category > Security.
5. Locate the Superuser Group property and change the value to the appropriate group name for your environment. For example, <hadoop>.
6. Click Save Changes to commit the changes.
7. Restart the HDFS service.
To enable your access to the superuser account now that Kerberos is enabled, you must now create a Kerberos principal or an Active Directory user whose first component is <hadoop>:

#########################################################
#### KERBEROS TICKET RENEWER ####
# Oozie and Hue require that the realm support renewable tickets
# Below is for only HUE
kadmin.local: modprinc -maxrenewlife 90day krbtgt/AWS.COM
kadmin.local: modprinc -maxrenewlife 90day +allow_renewable hue/ec2-34-123-123-123.compute-1.amazonaws.com@AWS.COM


#########################################################
----------- Enable Kerberos: Part 2 (using Wizard)



# Get or Create a Kerberos Principal for the Cloudera Manager Server
# If you have enabled YARN Resource Manager HA in your non-secure cluster, you should clear the StateStore znode in ZooKeeper before enabling Kerberos. 
kadmin:  addprinc -pw <Password> cloudera-scm/admin@YOUR-LOCAL-REALM.COM

## Before enable Kerberos 
# Cloudera Manager -> Administration -> Security -> Kerberos Credentials -> Configuration
a. Update REALM.COM
b. Update Host
c. Update Encryption Type
d. CM -> Security -> Enable Kerberos
e. Optional: Manage krb5.conf through Cloudera Manager (enable)-allows you to choose whether Cloudera Manager should deploy the krb5.conf on your cluster or not. If left unchecked, you must ensure that the krb5.conf is deployed on all hosts in the cluster, including the Cloudera Manager Server's host
# Cloudera Manager -> Administration -> Security -> Kerberos Credentials -> Import Kerberos Account manager Credentials
f. CM -> Security -> Kerberos Credentials -> Generate Missing Credentials
g. Optional: It will ask for Import Kerberos Account Manager credentials: Username: root/admin, pwd: hadoop1, realm: AWS.COM



#########################################################
#### ADD SENTRY SERVICE ####

# The Sentry service only uses HadoopUserGroup mappings. 

http://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_sg_sentry.html#xd_583c10bfdbd326ba--7f25092b-13fba2465e5--7f93


#########################################################
#### Pre-request ####
# Create a DB, user in Mysql.
create database sentry;create user 'sentry' identified by 'Hadoop1@'; grant all on sentry.* to sentry; flush privileges;

# Cloudera Manager
CM -> Add services -> Sentry -> Choose the node for Sentry Server & Gateway -> Choose the Customized Db (Mysql: port# 3306) -> 

#########################################################
#### Enable Sentry for Hue ####
Cloudera Manager -> Configuration -> Choose Sentry -> restart Hue


######################################################### -- old method
#### CREATE POLICY FILE - Sample 1 
## there is no sentry-provider.ini file to edit anymore ####
http://www.cloudera.com/documentation/enterprise/5-7-x/topics/cdh_sg_sentry.html#xd_583c10bfdbd326ba--7f25092b-13fba2465e5--7f93
https://vimeo.com/89523907
https://blogs.apache.org/sentry/entry/getting_started
http://gethue.com/apache-sentry-made-easy-with-the-new-hue-security-app/
http://www.yourtechchick.com/hadoop/no-databases-available-permissions-missing-error-hive-sentry/

## A single global policy file can be used to control access to an entire HiveServer2 instance, and multiple dependent per database policy files can be linked to the global one
# There are usually three sections in the global policy file
# A [groups] section that provides group-to-role mapping
# A [roles] section that provides role-to-privileges mapping
# A [databases] (optional) section that provides database-to-per-database policy file mapping. This allows for maintaining per-database privileges separately.

# Global policy file:
[groups]
admin_group = admin_role
dep1_admin = uri_role

[roles]
admin_role = server=server1
uri_role = hdfs:///ha-nn-uri/data

[databases]
db1 = hdfs://ha-nn-uri/user/hive/sentry/db1.ini

######################################################### -- old method
#### CREATE POLICY FILE - Sample 2 - there is no sentry-provider.ini file to edit anymore ####
>vi sentry-provider.ini
[groups]
management = manager_role
analyst = analyst_role, junior_analyst_role
jranalyst = junior_analyst_role
admin = admin_role

[roles]
manager_role = server=server1->db=default
analyst_role = server=server1->db=default->table=analystl_table->action=select
junior_analyst_role=server=server1->db=default->table=jranalyst1_table->action=select

# Implies everything on server1
admin_role=server=server1

######################################################### -- old method
#### PLACE POLICY FILE IN HDFS ####
>hadoop fs -put sentry-provider.ini /user/hive/sentry/sentry-provider.ini
>hadoop fs -chgrp hive /user/hive/sentry/sentry-provider.ini
>hadoop fs -ls /user/hive/sentry/sentry-provider.ini
>hadoop fs -chmod 640 /user/hive/sentry/sentry-provider.ini
>hadoop fs -ls /user/hive/sentry/sentry-provider.ini

#########################################################
#### UPDATE HUE.INI ####
## To have Hue point to a Sentry service and another host, modify these hue.ini properties:
# Hue will also automatically pick up the server name of HiveServer2 from the sentry-site.xml file of /etc/hive/conf

[libsentry]
  # Hostname or IP of server.
  hostname=localhost
 
  # Port the sentry service is running on.
  port=8038
 
  # Sentry configuration directory, where sentry-site.xml is located.
  sentry_conf_dir=/etc/sentry/conf

#########################################################
#### EDIT ROLES/PRIVILEGES IN HUE ####
  
# To be able to edit roles and privileges in Hue, the logged-in Hue user needs to belong to a group in Hue that is also an admin group in Sentry 
# (whatever UserGroupMapping Sentry is using, the corresponding groups must exist in Hue or need to be entered manually). 
# For example, our ‘hive’ user belongs to a ‘hive’ group in Hue and also to a ‘hive’ group in Sentry:
# Add the Hive, Impala and Hue groups to Sentry’s admin groups. 
# If an end user is in one of these admin groups, that user has administrative privileges on the Sentry Server

<property>
  <name>sentry.service.admin.group</name>
  <value>hive,impala,hue</value>
 </property>
   
#########################################################
#### SAMPLE USERS ####

# Make sure to sync Linux users/groups with Hue
hive (admin) belongs to the hive group
user1_1 belongs to the user_group1 group
user2_1 belongs to the user_group2 group
   
#########################################################
#### Make sure HUE is allowed to connect to Sentry ####
# If using Kerberos, make sure ‘hue’ is allowed to connect to Sentry in /etc/sentry/conf/sentry-site.xml:
<property>
    <name>sentry.service.allow.connect</name>
    <value>impala,hive,solr,hue</value>
</property>   
   
#########################################################   
# Note: In Sentry 1.5, you will need to specify a ‘entry.store.jdbc.password’ property in the sentry-site.xml, if not you will get:

Caused by: org.apache.sentry.provider.db.service.thrift.SentryConfigurationException: Error reading sentry.store.jdbc.password
   
#########################################################
#### HIVE WAREHOUSE ACCESS ####

# This will make sure only Hive and Impala can access the files in /user/hive/warehouse
>hadoop fs -chown -R hive:hive /user/hive/warehouse

# This will make sure only Hive user and group can only do the changes in /user/hive/warehouse
>hadoop fs -chmod -R 770 /user/hive/warehouse


#########################################################
#### ENABLE SENTRY IN HIVE via CM ####

# It should be done before enable sentry for Impala
Cloudera Manager -> hive -> Configuration -> Hive(Service-wide) -> Enable Sentry Authorization using Policy Files -> true (check)
Cloudera Manager -> hive -> Configuration -> Hive(Service-wide) -> Sentry Service -> Sentry
Cloudera Manager -> hive -> Configuration -> HiveServer2 -> HiveServer2 Enable Impersonation -> false (uncheck)
# Restart the Hive service now

#########################################################
#### ENABLE SENTRY IN IMPALA via CM ####

Cloudera manager -> Impala -> Configuration -> Imapal (service-wide) -> Enable Sentry Authorization -> trun (check)
# Restart the Impala service now

#########################################################


#########################################################
------------- Enable Sentry 
http://www.cloudera.com/documentation/enterprise/latest/topics/sg_sentry_service_config.html

# Enabling the Sentry Service Using Cloudera Manager
a. Before Enabling the Sentry Service
b. Enabling the Sentry Service for Hive

# Before Enabling the Sentry Service

Note: 
a. If you are going to enable HDFS/Sentry synchronization, you do not need to perform the following step to explicitly set permissions for the Hive warehouse directory. 
b. HDFS/Sentry synchronization is on hold in our case


# All files and subdirectories should be owned by hive:hive (/user/hive/warehouse)
1: If you have enabled Kerberos on your cluster, you must kinit as the hdfs user before you set permissions. For example:
	
	$ sudo -u hdfs kinit -kt <hdfs.keytab> hdfs # If this command is NOT working then try the below steps
	## When you enabled Kerberos for the HDFS service in Step 9, you lost access to the HDFS super user account via sudo -u hdfs commands. To enable your access to the HDFS super user account now that Kerberos is enabled, you must create a Kerberos principal whose first component is hdfs
	$ kadmin:  addprinc hdfs@AWS.COM
  	$ kinit hdfs@AWS.COM
	$ klist -e -k -t /etc/hadoop/conf/hdfs.keytab
	$ sudo -u hdfs kinit -kt /etc/hadoop/conf/hdfs.keytab hdfs@AWS.COM
	
	$ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/warehouse
	$ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/warehouse
	# Make sure  hive.warehouse.subdir.inherit.perms is true. So that sub directory will automatically use the above chown
	
	sudo -u hdfs kinit -kt /etc/hadoop/conf/hdfs.keytab hdfs@AWS.COM ## you must kinit as the hdfs user before you set permissions.
	hadoop fs -chmod -R 771 /user/hive/warehouse
	hadoop fs -chown -R hive:hive /user/hive/warehouse

1.1: Ignore this step.. just for reference:   The Hive warehouse directory (/user/hive/warehouse or any path you specify as hive.metastore.warehouse.dir in your hive-site.xml) must be owned by the Hive user and group.

make sure 771 on all subdirectories (for example, /user/hive/warehouse/mysubdir) ***
$ sudo -u hdfs kinit -kt <hdfs.keytab> hdfs
$ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/warehouse
$ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/warehouse

hdfs dfs -chown -R hive:hive /user/hive/warehouse/prod.db

## Note that when you update the default Hive warehouse, previously created tables will not be moved over automatically. Therefore, tables created before the update will remain at /user/hive/warehouse/<old_table>. However, after the update, any new tables created in the default location will be found at /data/<new_table>.

---------- create a user called HIVE (it is mandatory)


-----------

Test: 

insert into prod.test1 values (10000, 'Bob', 2016, 11);

---------------

Hive Impersonation is enabled for Hive Server2 role 'HiveServer2 (ec2-34-123-123-123)

HiveServer2 Enable Impersonation
hive.server2.enable.impersonation, hive.server2.enable.doAs

----------

Disable the existing Sentry policy file for any Hive, Impala, or Solr services on the cluster. To do this:
Go to the Hive, Impala, or Solr service.
Click the Configuration tab.
Select Scope > Service Name (Service-Wide).
Select Category > Policy File Based Sentry.
Clear Enable Sentry Authorization using Policy Files.

-------Securing the Hive Metastore
It's important that the Hive metastore be secured. If you want to override the Kerberos prerequisite for the Hive metastore, set the sentry.hive.testing.mode property to true to allow Sentry to work with weaker authentication mechanisms. Add the following property to the HiveServer2 and Hive metastore's sentry-site.xml:
<property>
  <name>sentry.hive.testing.mode</name>
  <value>true</value>
</property>
Impala does not require this flag to be set.

---Securing the Hive Metastore
--- hive -> configuration -> sentry
http://www.cloudera.com/documentation/enterprise/latest/topics/sg_sentry_service_config.html#concept_wjm_qxm_vq

Hive Service Advanced Configuration Snippet (Safety Valve) for sentry-site.xml

Add the following value
<property>
  <name>sentry.hive.testing.mode</name>
  <value>true</value>
</property>

----Hive Metastore Server Security Configuration
----Hive -> configuration
http://www.cloudera.com/documentation/enterprise/latest/topics/cdh_sg_hive_metastore_security.html

Hive Service Advanced Configuration Snippet (Safety Valve) for hive-site.xml

Add the following value
<property>
  <name>hive.metastore.sasl.enabled</name>
  <value>true</value>
  <description>If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.</description>
</property>

# Note: make sure to keep the /etc/hive/conf/hive.keytab file ready 
<property>
  <name>hive.metastore.kerberos.keytab.file</name>
  <value>/etc/hive/conf/hive.keytab</value>
  <description>The path to the Kerberos Keytab file containing the metastore thrift server's service principal.</description>
</property>

<property>
  <name>hive.metastore.kerberos.principal</name>
  <value>hive/_HOST@YOUR-REALM.COM</value>
  <description>The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.</description>
</property>

############################ Resource Management - Dynamic Resource pooling ####################
1. For resource (vcore, memory) to a particular job queue
2. You can create multiple job queues (high and low priority, etc).  high-priority job queue is for regular jobs triggered using batchID, low-priority job queue is for jobs triggered using individual user id – it will be considered as adhoc query. 
3. You can also have different weightage for high and low priority job queue during day time, night time, weekends, default. 
4. Use ‘default queue mapping’ , so that it will assign jobs triggered by batchID directly to high-prioity queue and individual user queries directly assigned to low-priority queue ‘irrespective’ of specifying the job queue

#################################################################################################

